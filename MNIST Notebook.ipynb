{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    #MNIST dataset is composed of monochrome 28x28 pixel images, so the desired shape for our input layer is [batch_size, 28, 28, 1]\n",
    "    input_layer = tf.reshape(features['image'], [-1, 28, 28, 1])\n",
    "    #The methods in the layers module for creating convolutional and pooling layers \n",
    "     \n",
    "    # Convolutional Layer #1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "          inputs=input_layer,\n",
    "          filters=32,\n",
    "          kernel_size=[5, 5],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional Layer #2 and Pooling Layer #2\n",
    "    conv2 = tf.layers.conv2d(\n",
    "          inputs=pool1,\n",
    "          filters=64,\n",
    "          kernel_size=[5, 5],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Dense Layer\n",
    "    #flatten our feature map (pool2) to shape [batch_size, features], so that our tensor has only two dimensions:\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "    #Each example has 7 (pool2 height) * 7 (pool2 width) * 64 (pool2 channels) features,\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(\n",
    "          inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "      # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "    predictions = {\n",
    "          # Generate predictions (for PREDICT and EVAL mode)\n",
    "          \"classes\": tf.argmax(input=logits, axis=1),\n",
    "          # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "          # `logging_hook`.\n",
    "          \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "      }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        export_outputs = {\n",
    "              'prediction': tf.estimator.export.PredictOutput(predictions)\n",
    "          }\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "              mode, predictions=predictions, export_outputs=export_outputs)\n",
    "        \n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "      # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "          \"accuracy\": tf.metrics.accuracy(\n",
    "              labels=labels, predictions=predictions[\"classes\"])\n",
    "      }\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "          mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and eval data\n",
    "((train_data, train_labels),\n",
    " (eval_data, eval_labels)) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_data = train_data/np.float32(255)\n",
    "train_labels = train_labels.astype(np.int32)  # not required\n",
    "\n",
    "eval_data = eval_data/np.float32(255)\n",
    "eval_labels = eval_labels.astype(np.int32)  # not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging for predictions\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors=tensors_to_log, every_n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 28\n",
    "WIDTH = 28\n",
    "NUM_CHANNELS = 1\n",
    "def serving_input_fn():\n",
    "    # Note: only handles one image at a time \n",
    "    feature_placeholders = {\"image_bytes\": tf.placeholder(dtype = tf.string, shape = [])}\n",
    "    image, _ = read_and_preprocess(\n",
    "        tf.squeeze(input = feature_placeholders[\"image_bytes\"]))\n",
    "    image[\"image\"] = tf.expand_dims(input = image[\"image\"], axis = 0)\n",
    "    return tf.estimator.export.ServingInputReceiver(features = image, receiver_tensors = feature_placeholders)\n",
    "  \n",
    "def read_and_preprocess(image_bytes, label = None, augment = False):\n",
    "    # Decode the image, end up with pixel values that are in the -1, 1 range\n",
    "    image = tf.image.decode_jpeg(contents = image_bytes, channels = NUM_CHANNELS)\n",
    "    image = tf.image.convert_image_dtype(image = image, dtype = tf.float32) # 0-1\n",
    "    image = tf.expand_dims(input = image, axis = 0) # resize_bilinear needs batches\n",
    "    \n",
    "    if augment:\n",
    "        image = tf.image.resize_bilinear(images = image, size = [HEIGHT + 10, WIDTH + 10], align_corners = False)\n",
    "        image = tf.squeeze(input = image, axis = 0) # remove batch dimension\n",
    "        image = tf.random_crop(value = image, size = [HEIGHT, WIDTH, NUM_CHANNELS])\n",
    "        image = tf.image.random_flip_left_right(image = image)\n",
    "        image = tf.image.random_brightness(image = image, max_delta = 63.0 / 255.0)\n",
    "        image = tf.image.random_contrast(image = image, lower = 0.2, upper = 1.8)\n",
    "    else:\n",
    "        image = tf.image.resize_bilinear(images = image, size = [HEIGHT, WIDTH], align_corners = False)\n",
    "        image = tf.squeeze(input = image, axis = 0) # remove batch dimension\n",
    "        \n",
    "    # Pixel values are in range [0,1], convert to [-1,1]\n",
    "    image = tf.subtract(x = image, y = 0.5)\n",
    "    image = tf.multiply(x = image, y = 2.0)\n",
    "    return {\"image\": image}, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0707 08:04:21.899794 140624835032832 estimator.py:1790] Using default config.\n",
      "I0707 08:04:21.902186 140624835032832 estimator.py:209] Using config: {'_log_step_count_steps': 100, '_experimental_distribute': None, '_num_worker_replicas': 1, '_evaluation_master': '', '_protocol': None, '_eval_distribute': None, '_is_chief': True, '_keep_checkpoint_max': 5, '_device_fn': None, '_global_id_in_cluster': 0, '_master': '', '_task_type': 'worker', '_model_dir': 'mnist_convnet_model', '_save_summary_steps': 100, '_tf_random_seed': None, '_save_checkpoints_steps': None, '_train_distribute': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe504c19a58>, '_keep_checkpoint_every_n_hours': 10000, '_task_id': 0, '_num_ps_replicas': 0, '_service': None, '_experimental_max_worker_delay_secs': None, '_save_checkpoints_secs': 600}\n"
     ]
    }
   ],
   "source": [
    "mnist_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn, model_dir=\"mnist_convnet_model\")\n",
    "def train_and_evaluate(output_dir, num_train_steps):\n",
    "    \n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"image\": train_data},y=train_labels,batch_size=100,num_epochs=None,shuffle=True)\n",
    "  \n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"image\": eval_data},y=eval_labels,num_epochs=1,shuffle=False)\n",
    "\n",
    "    train_spec=tf.estimator.TrainSpec(input_fn = train_input_fn, max_steps = num_train_steps)\n",
    "    #read_dataset('./train.csv',mode = tf.estimator.ModeKeys.TRAIN)\n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "    eval_spec=tf.estimator.EvalSpec(\n",
    "                       input_fn = eval_input_fn,\n",
    "                       steps = None,\n",
    "                       start_delay_secs = 1, # start evaluating after N seconds\n",
    "                       throttle_secs = 10,  # evaluate every N seconds\n",
    "                       exporters = exporter)\n",
    "    tf.estimator.train_and_evaluate(mnist_classifier, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0707 08:04:40.234121 140624835032832 estimator_training.py:186] Not using Distribute Coordinator.\n",
      "I0707 08:04:40.235668 140624835032832 training.py:612] Running training and evaluation locally (non-distributed).\n",
      "I0707 08:04:40.236810 140624835032832 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "I0707 08:04:40.271170 140624835032832 estimator.py:1145] Calling model_fn.\n",
      "I0707 08:04:40.484861 140624835032832 estimator.py:1147] Done calling model_fn.\n",
      "I0707 08:04:40.489242 140624835032832 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "I0707 08:04:40.576490 140624835032832 monitored_session.py:240] Graph was finalized.\n",
      "I0707 08:04:40.683209 140624835032832 session_manager.py:500] Running local_init_op.\n",
      "I0707 08:04:40.689953 140624835032832 session_manager.py:502] Done running local_init_op.\n",
      "I0707 08:04:40.911427 140624835032832 basic_session_run_hooks.py:606] Saving checkpoints for 0 into mnist_convnet_model/model.ckpt.\n",
      "I0707 08:04:41.344206 140624835032832 basic_session_run_hooks.py:262] loss = 2.3285804, step = 1\n",
      "I0707 08:04:58.747669 140624835032832 basic_session_run_hooks.py:692] global_step/sec: 5.74589\n",
      "I0707 08:04:58.755350 140624835032832 basic_session_run_hooks.py:260] loss = 2.2941854, step = 101 (17.411 sec)\n",
      "I0707 08:05:14.867452 140624835032832 basic_session_run_hooks.py:692] global_step/sec: 6.20362\n",
      "I0707 08:05:14.874584 140624835032832 basic_session_run_hooks.py:260] loss = 2.286793, step = 201 (16.119 sec)\n",
      "I0707 08:05:31.125601 140624835032832 basic_session_run_hooks.py:692] global_step/sec: 6.1507\n",
      "I0707 08:05:31.135405 140624835032832 basic_session_run_hooks.py:260] loss = 2.2506592, step = 301 (16.261 sec)\n",
      "I0707 08:05:47.945709 140624835032832 basic_session_run_hooks.py:692] global_step/sec: 5.94527\n",
      "I0707 08:05:47.954795 140624835032832 basic_session_run_hooks.py:260] loss = 2.2468576, step = 401 (16.819 sec)\n",
      "I0707 08:06:04.977929 140624835032832 basic_session_run_hooks.py:692] global_step/sec: 5.87122\n",
      "I0707 08:06:04.986250 140624835032832 basic_session_run_hooks.py:260] loss = 2.222905, step = 501 (17.031 sec)\n",
      "I0707 08:06:21.535891 140624835032832 basic_session_run_hooks.py:692] global_step/sec: 6.03939\n",
      "I0707 08:06:21.544106 140624835032832 basic_session_run_hooks.py:260] loss = 2.188855, step = 601 (16.558 sec)\n",
      "I0707 08:06:37.898438 140624835032832 basic_session_run_hooks.py:692] global_step/sec: 6.11152\n",
      "I0707 08:06:37.906217 140624835032832 basic_session_run_hooks.py:260] loss = 2.1492143, step = 701 (16.362 sec)\n",
      "I0707 08:06:53.846583 140624835032832 basic_session_run_hooks.py:692] global_step/sec: 6.27032\n",
      "I0707 08:06:53.853779 140624835032832 basic_session_run_hooks.py:260] loss = 2.1235056, step = 801 (15.948 sec)\n",
      "I0707 08:07:09.793697 140624835032832 basic_session_run_hooks.py:692] global_step/sec: 6.27072\n",
      "I0707 08:07:09.802524 140624835032832 basic_session_run_hooks.py:260] loss = 2.0598252, step = 901 (15.949 sec)\n",
      "I0707 08:07:26.225146 140624835032832 basic_session_run_hooks.py:692] global_step/sec: 6.08588\n",
      "I0707 08:07:26.228161 140624835032832 basic_session_run_hooks.py:260] loss = 1.9526243, step = 1001 (16.426 sec)\n",
      "I0707 08:07:43.143921 140624835032832 basic_session_run_hooks.py:692] global_step/sec: 5.9106\n",
      "I0707 08:07:43.153854 140624835032832 basic_session_run_hooks.py:260] loss = 1.8930811, step = 1101 (16.926 sec)\n",
      "I0707 08:08:00.590332 140624835032832 basic_session_run_hooks.py:692] global_step/sec: 5.73184\n",
      "I0707 08:08:00.593729 140624835032832 basic_session_run_hooks.py:260] loss = 1.8313774, step = 1201 (17.440 sec)\n",
      "I0707 08:08:17.727956 140624835032832 basic_session_run_hooks.py:692] global_step/sec: 5.8351\n",
      "I0707 08:08:17.736709 140624835032832 basic_session_run_hooks.py:260] loss = 1.5861131, step = 1301 (17.143 sec)\n",
      "I0707 08:08:35.249796 140624835032832 basic_session_run_hooks.py:692] global_step/sec: 5.70717\n",
      "I0707 08:08:35.256150 140624835032832 basic_session_run_hooks.py:260] loss = 1.3757529, step = 1401 (17.519 sec)\n",
      "I0707 08:08:51.277596 140624835032832 basic_session_run_hooks.py:692] global_step/sec: 6.23916\n",
      "I0707 08:08:51.280734 140624835032832 basic_session_run_hooks.py:260] loss = 1.3967949, step = 1501 (16.025 sec)\n",
      "I0707 08:09:08.213561 140624835032832 basic_session_run_hooks.py:692] global_step/sec: 5.9046\n",
      "I0707 08:09:08.220542 140624835032832 basic_session_run_hooks.py:260] loss = 1.0156643, step = 1601 (16.940 sec)\n",
      "I0707 08:09:24.940518 140624835032832 basic_session_run_hooks.py:692] global_step/sec: 5.97837\n",
      "I0707 08:09:24.948419 140624835032832 basic_session_run_hooks.py:260] loss = 0.89807147, step = 1701 (16.728 sec)\n",
      "I0707 08:09:42.218795 140624835032832 basic_session_run_hooks.py:692] global_step/sec: 5.78761\n",
      "I0707 08:09:42.227342 140624835032832 basic_session_run_hooks.py:260] loss = 0.86347747, step = 1801 (17.279 sec)\n",
      "I0707 08:09:59.511940 140624835032832 basic_session_run_hooks.py:692] global_step/sec: 5.78263\n",
      "I0707 08:09:59.520123 140624835032832 basic_session_run_hooks.py:260] loss = 0.9326377, step = 1901 (17.293 sec)\n",
      "I0707 08:10:16.188702 140624835032832 basic_session_run_hooks.py:606] Saving checkpoints for 2000 into mnist_convnet_model/model.ckpt.\n",
      "I0707 08:10:16.369873 140624835032832 estimator.py:1145] Calling model_fn.\n",
      "I0707 08:10:16.501127 140624835032832 estimator.py:1147] Done calling model_fn.\n",
      "I0707 08:10:16.735467 140624835032832 evaluation.py:255] Starting evaluation at 2019-07-07T08:10:16Z\n",
      "I0707 08:10:16.829653 140624835032832 monitored_session.py:240] Graph was finalized.\n",
      "I0707 08:10:16.835340 140624835032832 saver.py:1280] Restoring parameters from mnist_convnet_model/model.ckpt-2000\n",
      "I0707 08:10:16.909845 140624835032832 session_manager.py:500] Running local_init_op.\n",
      "I0707 08:10:16.929191 140624835032832 session_manager.py:502] Done running local_init_op.\n",
      "I0707 08:10:22.392062 140624835032832 evaluation.py:275] Finished evaluation at 2019-07-07-08:10:22\n",
      "I0707 08:10:22.396352 140624835032832 estimator.py:2039] Saving dict for global step 2000: accuracy = 0.86, global_step = 2000, loss = 0.651386\n",
      "I0707 08:10:22.399221 140624835032832 estimator.py:2099] Saving 'checkpoint_path' summary for global step 2000: mnist_convnet_model/model.ckpt-2000\n",
      "I0707 08:10:22.452593 140624835032832 estimator.py:1145] Calling model_fn.\n",
      "I0707 08:10:22.542483 140624835032832 estimator.py:1147] Done calling model_fn.\n",
      "I0707 08:10:22.544040 140624835032832 export_utils.py:170] Signatures INCLUDED in export for Predict: ['prediction', 'serving_default']\n",
      "I0707 08:10:22.545048 140624835032832 export_utils.py:170] Signatures INCLUDED in export for Train: None\n",
      "I0707 08:10:22.545736 140624835032832 export_utils.py:170] Signatures INCLUDED in export for Regress: None\n",
      "I0707 08:10:22.546469 140624835032832 export_utils.py:170] Signatures INCLUDED in export for Eval: None\n",
      "I0707 08:10:22.547182 140624835032832 export_utils.py:170] Signatures INCLUDED in export for Classify: None\n",
      "I0707 08:10:22.579763 140624835032832 saver.py:1280] Restoring parameters from mnist_convnet_model/model.ckpt-2000\n",
      "I0707 08:10:22.618736 140624835032832 builder_impl.py:661] Assets added to graph.\n",
      "I0707 08:10:22.620164 140624835032832 builder_impl.py:456] No assets to write.\n",
      "I0707 08:10:22.676789 140624835032832 builder_impl.py:421] SavedModel written to: mnist_convnet_model/export/exporter/temp-b'1562487022'/saved_model.pb\n",
      "I0707 08:10:22.721906 140624835032832 estimator.py:368] Loss for final step: 0.76729447.\n"
     ]
    }
   ],
   "source": [
    "# Run training  \n",
    "import shutil\n",
    "OUTDIR = 'mnist_convnet_model'\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "train_and_evaluate(OUTDIR, num_train_steps = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'cpb100-245606' # REPLACE WITH YOUR PROJECT ID\n",
    "REGION = 'us-east1' \n",
    "BUCKET = 'my_demo_model' # REPLACE WITH YOUR bucket name\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '1.4' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authorizing the Cloud ML Service account service-578381499295@cloud-ml.google.com.iam.gserviceaccount.com to access files in my_demo_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   233    0   233    0     0    665      0 --:--:-- --:--:-- --:--:--   665\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%bash\n",
    "PROJECT_ID=$PROJECT\n",
    "AUTH_TOKEN=$(gcloud auth print-access-token)\n",
    "SVC_ACCOUNT=$(curl -X GET -H \"Content-Type: application/json\" \\\n",
    "    -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n",
    "    https://ml.googleapis.com/v1/projects/${PROJECT_ID}:getConfig \\\n",
    "    | python -c \"import json; import sys; response = json.load(sys.stdin); \\\n",
    "    print(response['serviceAccount'])\")\n",
    "\n",
    "echo \"Authorizing the Cloud ML Service account $SVC_ACCOUNT to access files in $BUCKET\"\n",
    "#gsutil -m defacl ch -u $SVC_ACCOUNT:R gs://$BUCKET\n",
    "#gsutil -m acl ch -u $SVC_ACCOUNT:R -r gs://$BUCKET  # error message (if bucket is empty) can be ignored\n",
    "#gsutil -m acl ch -u $SVC_ACCOUNT:W gs://$BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_demo_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file:///home/jupyter/mnist_convnet_model/export/exporter/1562487022/saved_model.pb [Content-Type=application/octet-stream]...\n",
      "/ [1/1 files][ 23.4 KiB/ 23.4 KiB] 100% Done                                    \n",
      "Operation completed over 1 objects/23.4 KiB.                                     \n",
      "Copying file:///home/jupyter/mnist_convnet_model/export/exporter/1562487022/variables/variables.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/mnist_convnet_model/export/exporter/1562487022/variables/variables.index [Content-Type=application/octet-stream]...\n",
      "-\n",
      "Operation completed over 2 objects/12.5 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo $BUCKET\n",
    "#Upload saved model to bucket, replace 1562483410 with your subfolder containing the latest saved model\n",
    "gsutil -m cp ${PWD}/mnist_convnet_model/export/exporter/1562487022/*.* gs://${BUCKET}/mnist_convnet_model/\n",
    "gsutil -m cp ${PWD}/mnist_convnet_model/export/exporter/1562487022/variables/*.* gs://${BUCKET}/mnist_convnet_model/variables/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0707 08:19:53.338560 140624835032832 estimator.py:1145] Calling model_fn.\n",
      "I0707 08:19:53.426802 140624835032832 estimator.py:1147] Done calling model_fn.\n",
      "I0707 08:19:53.512462 140624835032832 monitored_session.py:240] Graph was finalized.\n",
      "I0707 08:19:53.515906 140624835032832 saver.py:1280] Restoring parameters from mnist_convnet_model/model.ckpt-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected class:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0707 08:19:53.581781 140624835032832 session_manager.py:500] Running local_init_op.\n",
      "I0707 08:19:53.587033 140624835032832 session_manager.py:502] Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probabilities': array([0.03396588, 0.00908962, 0.17241697, 0.0268354 , 0.11940932,\n",
      "       0.04745677, 0.47018716, 0.02021108, 0.05498011, 0.0454477 ],\n",
      "      dtype=float32), 'classes': 6}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(eval_data[100])\n",
    "pred = mnist_classifier.predict(input_fn=tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"image\": eval_data[100]},\n",
    "      shuffle=False))\n",
    "\n",
    "print('expected class: ',eval_labels[100])\n",
    "for p in pred:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
